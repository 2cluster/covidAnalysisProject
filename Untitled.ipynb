{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ipython's max row display\n",
    "pd.set_option('display.max_row', 1000)\n",
    "\n",
    "# Set iPython's max column width to 50\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df  = pd.read_csv('./data/test.csv')\n",
    "subs_df  = pd.read_csv('./data/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id County Province_State Country_Region  Population    Weight        Date  \\\n",
      "0   1    NaN            NaN    Afghanistan    27657145  0.058359  2020-01-23   \n",
      "1   2    NaN            NaN    Afghanistan    27657145  0.583587  2020-01-23   \n",
      "2   3    NaN            NaN    Afghanistan    27657145  0.058359  2020-01-24   \n",
      "3   4    NaN            NaN    Afghanistan    27657145  0.583587  2020-01-24   \n",
      "4   5    NaN            NaN    Afghanistan    27657145  0.058359  2020-01-25   \n",
      "\n",
      "           Target  TargetValue  \n",
      "0  ConfirmedCases          0.0  \n",
      "1      Fatalities          0.0  \n",
      "2  ConfirmedCases          0.0  \n",
      "3      Fatalities          0.0  \n",
      "4  ConfirmedCases          0.0  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location restructure\n",
    "\n",
    "Voor de locaties gelden dat er in de dataset met verschillende granulatiteit word gewerkt, zowel \n",
    "1. USA\n",
    "2. USA, New York\n",
    "3. USA , New York, New York \n",
    "\n",
    "komt voor in de dataset.\n",
    "\n",
    "Dit zorgt ervoor dat als we USA selecteren we 3x dezelfde data selecteren.<br>\n",
    "We willen zo min mogelijk knippen in de dataset dus voegen we een kolom toe om makkelijk een subselectie te kunnen maken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "either both or neither of x and y should be given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dde52c900eec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# replace location-columns by unique indentifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'level'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'County'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'level'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Province_State'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: either both or neither of x and y should be given"
     ]
    }
   ],
   "source": [
    "# replace location-columns by unique indentifiers \n",
    "train_df['level'] = np.where(pd.notnull(train_df[['County']]), 3, 1)\n",
    "train_df['level'] = np.where(pd.notnull(train_df[['Province_State']]), 2)\n",
    "train_df = train_df.replace(np.nan, '', regex=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_df['location_combined'] = train_df['Country_Region'] + \"-\" + train_df['Province_State'] + \"-\" + train_df['County']\n",
    "# train_df = train_df.drop(['Province_State','County','Country_Region','Id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_filter = train_df[\"level\"]==2\n",
    "\n",
    "train_df.where(level_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date restructure\n",
    "\n",
    "Om landen in de tijd te kunnen analyseren moeten we zorgen dat de dagen de juiste schaal hebben.\n",
    "\n",
    "Dit betekend dat we geen string maar een int gaan gebruiken. we gebruiken de eerste dag en de laatste dag die voorkomt in de dataset om te bepalen wat de range word, hier loop-en we doorheen om te kijken welke dagen voorkomen in de dataset. Met deze dagen bouwen we een dict op om de nieuwe index toe te passen \n",
    "\n",
    "> nergens word gehardcode zodat wanneer de dataset meer datums bevat de code blijft werken\n",
    "\n",
    "> we gaan ervanuit dat er geen datums tussen de eerste dag en de laatste dag bestaan die in de dataset missen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - verwijderen om als int te kunnen casten\n",
    "train_df[\"Date\"] = train_df[\"Date\"].replace({'-': ''}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kolom casten naar int\n",
    "\n",
    "train_df[\"Date\"] = pd.to_numeric(train_df[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wat is de range van datums\n",
    "\n",
    "min_date = train_df['Date'].min()\n",
    "max_date = train_df['Date'].max()\n",
    "print(min_date)\n",
    "print(max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_values = \"\"\n",
    "custom_index = 0\n",
    "\n",
    "for i in range(min_date, (max_date+1)):\n",
    "    \n",
    "    # komt het getal in de dataset voor?\n",
    "    if int(i) in train_df['Date'].unique():\n",
    "        \n",
    "        # ja, voeg een entry toe aan de dict <   20200101: 1,   >\n",
    "        dict_values += str(i) + \": \" + str(custom_index) + \", \"\n",
    "        custom_index+=1\n",
    "        \n",
    "exec(\"date_dict = {\" + dict_values + \"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Date'] = train_df['Date'].replace(date_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_locations = train_df.location_id.unique()\n",
    "unique_targets = train_df.Target.unique()\n",
    "\n",
    "print(\"There are \"+str(len(unique_locations))+\"\\t unique locations\")\n",
    "print(\"There are \"+str(len(unique_targets))+\"\\t unique Targets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in unique_locations:\n",
    "    for tar in unique_targets:\n",
    "        \n",
    "        filter1 = train_df[\"Target\"]==tar\n",
    "        filter2 = train_df[\"location_id\"]==loc\n",
    "        \n",
    "        temp = train_df.where(filter1)\n",
    "        temp = train_df.where(filter2)\n",
    "        \n",
    "        \n",
    "        continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
